{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rollup_generator_dev\n",
    "\n",
    "This is the dev notebook for the rollup generator script.\n",
    "\n",
    "First thing first, let's see how the MTA archives are set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile('/Users/alex/Downloads/201906.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20190601.zip',\n",
       " '20190602.zip',\n",
       " '20190603.zip',\n",
       " '20190604.zip',\n",
       " '20190605.zip',\n",
       " '20190606.zip',\n",
       " '20190607.zip',\n",
       " '20190608.zip',\n",
       " '20190609.zip',\n",
       " '20190610.zip',\n",
       " '20190611.zip',\n",
       " '20190612.zip',\n",
       " '20190613.zip',\n",
       " '20190614.zip',\n",
       " '20190615.zip',\n",
       " '20190616.zip',\n",
       " '20190617.zip',\n",
       " '20190618.zip',\n",
       " '20190619.zip',\n",
       " '20190620.zip',\n",
       " '20190621.zip',\n",
       " '20190622.zip',\n",
       " '20190623.zip',\n",
       " '20190624.zip',\n",
       " '20190625.zip',\n",
       " '20190626.zip',\n",
       " '20190627.zip',\n",
       " '20190628.zip',\n",
       " '20190629.zip',\n",
       " '20190630.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = zipfile.ZipFile(z.open('20190601.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7353        gtfs_j_20190601_092434.gtfs\n",
       "21569     gtfs_ace_20190601_210034.gtfs\n",
       "724         gtfs_g_20190601_044646.gtfs\n",
       "22305       gtfs_L_20190601_213845.gtfs\n",
       "23883       gtfs_L_20190601_230215.gtfs\n",
       "19315     gtfs_ace_20190601_190258.gtfs\n",
       "13961       gtfs_g_20190601_143345.gtfs\n",
       "2957     gtfs_bdfm_20190601_061631.gtfs\n",
       "11341     gtfs_ace_20190601_123119.gtfs\n",
       "281         gtfs_g_20190601_042920.gtfs\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.Series(zz.namelist()).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the MTA GTFS-RT archive is per-month, and is in the form of a zip of zips, where the second layer of zips is snapshots per feed per day per minute. Weirdly the feed IDs that you read from the API are numeric, whilst the feed IDs in the archive are in terms of the train lines they cover.\n",
    "\n",
    "This structure means that it is most efficient to parse a month's worth of data at time. The compressed file is 34 GB; the uncompressed file is much larger, but there's no reason to uncompress everything, as it's much more efficient to uncompress slices of the data, and work in batches.\n",
    "\n",
    "Although the archival data is provided in monthly form, it makes the most sense to provide the output data in daily form. This is a much more workable slice of data if you want to e.g. experiment, and it's easy to download a bunch of different objects with a for loop.\n",
    "\n",
    "Unforunately the only way to detect a trip that started in the 0:00-00:01 interval is to read in the 23:59-00:00 interval from the previous day, which requires downloading the entire previous month's worth of data for a single record.\n",
    "\n",
    "The same problem occurs on the opposite, tail end. Since we are using ragged-right time intervals (\"every trip that started on day X\"), trips will carry over into the following day. In the case of the last day of the month, this will require also reading in the first few hours worth of data from the folllowing day, which will require downloading *that* whole month's worth of data.\n",
    "\n",
    "So overall, to generate a month's worth of rollups for, say, June, we would also need to download all of May's data and all of July's data. This does unfortunately mean that we have a lag period of one month, plus however long it takes the MTA to upload the archives.\n",
    "\n",
    "All of this will raise the time required to do a parse run, but probably not by that much, seeing as how the total time cost will be compute-dominated anyway.\n",
    "\n",
    "Let's next consider some edge cases. The time appears to be in hourly EST timestamps, but let's make absolutely sure they aren't using UTC or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code taken from the gtfs-tripify codebase\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "\n",
    "def parse_feed(bytes):\n",
    "    \"\"\"\n",
    "    Helper function for reading a feed in using Protobuf. \n",
    "    Handles bad feeds by replacing them with None.\n",
    "    \"\"\"\n",
    "    # TODO: tests.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"error\")\n",
    "\n",
    "        try:\n",
    "            fm = gtfs_realtime_pb2.FeedMessage()\n",
    "            fm.ParseFromString(bytes)\n",
    "            return fm\n",
    "\n",
    "        # Protobuf occasionally raises an unexpected tag RuntimeWarning. This occurs when a\n",
    "        # feed that we read has unexpected problems, but is still valid overall. This \n",
    "        # warning corresponds with data loss in most cases. `gtfs-tripify` is sensitive to the\n",
    "        # disappearance of trips in the record. If data is lost, it's best to excise the \n",
    "        # message entirely. Hence we catch these warnings and return a flag value None, to be\n",
    "        # taken into account upstream. For further information see the following thread:\n",
    "        # https://groups.google.com/forum/#!msg/mtadeveloperresources/9Fb4SLkxBmE/BlmaHWbfw6kJ\n",
    "        except RuntimeWarning:\n",
    "            warnings.warn(\n",
    "                f\"The Protobuf parser raised a RunTimeWarning while parsing an update, indicating \"\n",
    "                f\"possible corruption and/or loss of data. This update cannot be safely used \"\n",
    "                f\"upstream and has been dropped.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # Raise for all other errors.\n",
    "        except:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zipfile.ZipExtFile name='gtfs_L_20190601_035944.gtfs' mode='r'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.open(zz.namelist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1559375984"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_feed(zz.open(zz.namelist()[0], 'r').read()).header.timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample archival file purports to be for the timestamp `20190601_035944`. That translates to June 1st, 2019 at 03:59:44 local time (e.g. 3:59 AM, the 44th second thereof). The Unix timestamp in the GTFS-RT file maps to `06/01/2019 @ 7:59am` exactly, per [this website](https://www.unixtimestamp.com/index.php). Interestingly enough, it appears that the least significant digit in the time included in the file name is the second, whilst the least significant digit in the time included in the GTFS-RT message itself is the minute, e.g. the second has been stripped off. Anyway, the time does match up to EST.\n",
    "\n",
    "Storing data in timezone-aware format like this is risky. Phenomena like daylight savings time and leap seconds can make this time ambiguous. Unix time, on the other hand, is unambiguous, so the time information on the file itself is what we will trust when we are attempting to order the buffers.\n",
    "\n",
    "I think we can trust the coverage of the files (e.g. they really do cover \"the whole month\"). Note that the exact amount of time covered in a month may differ due to time differences. It is also possible that some files will be missing outright, we need to account for this possibility in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 3, 31, 23, 59, tzinfo=tzfile('/usr/share/zoneinfo/America/New_York'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "YEARMONTH = '201904'\n",
    "\n",
    "datetime(int(YEARMONTH[:4]), int(YEARMONTH[-2:].lstrip('0')), 1, \n",
    "         tzinfo=tz.gettz('America/New_York')) - timedelta(minutes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the list of files, \n",
    "We need to parse the names of the files in order to:\n",
    "* Sort them into feed categories.\n",
    "* Determine their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameparts = []\n",
    "for n in zz.namelist():\n",
    "    assert 'gtfs_' in n and '.gtfs' in n\n",
    "    n = n.replace('gtfs_', '')[:-5]\n",
    "    lines_covered = n[:n.find('_')].upper()\n",
    "    nameparts.append(lines_covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7       4542\n",
       "G       4293\n",
       "J       4258\n",
       "NQRW    3431\n",
       "ACE     3301\n",
       "BDFM    3102\n",
       "L       2399\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(nameparts).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is odd. Many of the train lines are missing. Elsewhere, there are differences between the lines purported to by covered by the feed in the feed name, and the lines actually covered in the direct-access API. The feedmap for the API below for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEEDMAP = {\n",
    "    1:  ['1', '2', '3', '4', '5', '6', 'GS'],\n",
    "    2:  ['L'],\n",
    "    11: ['SI'],\n",
    "    16: ['N', 'Q', 'R', 'W'],\n",
    "    21: ['B', 'D', 'F', 'M'],\n",
    "    26: ['A', 'C', 'E', 'H', 'FS'],\n",
    "    31: ['G'],\n",
    "    36: ['J', 'Z'],\n",
    "    51: ['7']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 1.55 s, total: 1min 30s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_vehicle_update(message):\n",
    "    return str(message.trip_update.trip.route_id) == '' and str(message.alert) == ''\n",
    "\n",
    "def is_alert(message):\n",
    "    return str(message.alert) != ''\n",
    "\n",
    "def is_trip_update(message):\n",
    "    return not is_vehicle_update(message) and not is_alert(message)\n",
    "\n",
    "trip_id_unassigned_count = 0\n",
    "parse_error_count = 0\n",
    "msg_lines = []\n",
    "\n",
    "for n in zz.namelist():\n",
    "    msg_line = []\n",
    "    try:\n",
    "        buffer = parse_feed(zz.open(n, 'r').read())\n",
    "    except:  # parsing failed, bad message, skip it\n",
    "        parse_error_count += 1\n",
    "        continue\n",
    "    for entity in buffer.entity:\n",
    "        if is_trip_update(entity):\n",
    "            if entity.trip_update.trip.route_id == \"\":\n",
    "                trip_id_unassigned_count += 1\n",
    "            msg_line.append(entity.trip_update.trip.route_id)\n",
    "    msg_lines.append(msg_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_id_unassigned_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines_represented = []\n",
    "for msg_line in msg_lines:\n",
    "    train_lines_represented.append(''.join(np.sort(pd.Series(msg_line).unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7         4339\n",
       "G         4292\n",
       "J         4258\n",
       "NQR       3361\n",
       "DFM       3089\n",
       "L         2399\n",
       "ACEH      2039\n",
       "ACEFSH    1154\n",
       "77X        203\n",
       "AEFSH      107\n",
       "DNQR        70\n",
       "DFMR        13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_lines_represented).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, not only are the 1, 2, 3, 4, 5, 6 trains still missing, but the W is as well, and the R is somehow showing up in the `DFM` feed instead of the `NQR` feed, where it belongs, on certain occassions.\n",
    "\n",
    "It appears that the feeds that are not included are straight-up missing from the rollup. This must be a data error on the MTA's part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25326, 25324)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zz.namelist()), len(msg_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different day from a different month (the different month is not necessary, but I had another month's of data available so why not). Say, Febuary 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 3.18 s, total: 1min 28s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "zz = zipfile.ZipFile(zipfile.ZipFile('/Users/alex/Downloads/201902.zip').open('20190201.zip'))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_vehicle_update(message):\n",
    "    return str(message.trip_update.trip.route_id) == '' and str(message.alert) == ''\n",
    "\n",
    "def is_alert(message):\n",
    "    return str(message.alert) != ''\n",
    "\n",
    "def is_trip_update(message):\n",
    "    return not is_vehicle_update(message) and not is_alert(message)\n",
    "\n",
    "trip_id_unassigned_count = 0\n",
    "parse_error_count = 0\n",
    "msg_lines = []\n",
    "\n",
    "for n in zz.namelist():\n",
    "    msg_line = []\n",
    "    try:\n",
    "        buffer = parse_feed(zz.open(n, 'r').read())\n",
    "    except:  # parsing failed, bad message, skip it\n",
    "        parse_error_count += 1\n",
    "        continue\n",
    "    for entity in buffer.entity:\n",
    "        if is_trip_update(entity):\n",
    "            if entity.trip_update.trip.route_id == \"\":\n",
    "                trip_id_unassigned_count += 1\n",
    "            msg_line.append(entity.trip_update.trip.route_id)\n",
    "    msg_lines.append(msg_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_id_unassigned_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G         4027\n",
       "J         2988\n",
       "77X       2707\n",
       "ACEFSH    2676\n",
       "NQRW      2653\n",
       "BDFM      2421\n",
       "7         1527\n",
       "JZ         955\n",
       "           675\n",
       "NQR        315\n",
       "DFM        248\n",
       "AEFSH      115\n",
       "ACEH        76\n",
       "A            9\n",
       "ACE          7\n",
       "AC           6\n",
       "NW           4\n",
       "NQW          4\n",
       "Q            3\n",
       "BDF          2\n",
       "ACFSH        1\n",
       "BD           1\n",
       "DF           1\n",
       "NRW          1\n",
       "ACEFS        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines_represented = []\n",
    "for msg_line in msg_lines:\n",
    "    train_lines_represented.append(''.join(np.sort(pd.Series(msg_line).unique())))\n",
    "    \n",
    "pd.Series(train_lines_represented).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The W reports in this one, but the 1,2,3,4,5,6 are still missing.\n",
    "\n",
    "Let's look at a couple more days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(zz):\n",
    "    parse_error_count = 0\n",
    "    trip_id_unassigned_count = 0\n",
    "    \n",
    "    for n in zz.namelist():\n",
    "        msg_line = []\n",
    "        try:\n",
    "            buffer = parse_feed(zz.open(n, 'r').read())\n",
    "        except:  # parsing failed, bad message, skip it\n",
    "            parse_error_count += 1\n",
    "            continue\n",
    "        for entity in buffer.entity:\n",
    "            if is_trip_update(entity):\n",
    "                if entity.trip_update.trip.route_id == \"\":\n",
    "                    trip_id_unassigned_count += 1\n",
    "                msg_line.append(entity.trip_update.trip.route_id)\n",
    "        msg_lines.append(msg_line)\n",
    "        \n",
    "    train_lines_represented = []\n",
    "    for msg_line in msg_lines:\n",
    "        train_lines_represented.append(''.join(np.sort(pd.Series(msg_line).unique())))\n",
    "    return trip_id_unassigned_count, parse_error_count, pd.Series(train_lines_represented).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_id_unassigned_count, parse_error_count, train_lines_represented = report(\n",
    "    zipfile.ZipFile(z.open('20190615.zip'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G         8355\n",
       "J         7241\n",
       "ACEFSH    5812\n",
       "7         5743\n",
       "NQR       3724\n",
       "DFM       3310\n",
       "77X       2967\n",
       "NQRW      2653\n",
       "L         2440\n",
       "BDFM      2421\n",
       "JZ         955\n",
       "           689\n",
       "AEFSH      277\n",
       "ACEH        88\n",
       "A            9\n",
       "ACE          7\n",
       "AC           6\n",
       "NQW          4\n",
       "NW           4\n",
       "Q            3\n",
       "BDF          2\n",
       "ACFSH        1\n",
       "BD           1\n",
       "ACEFS        1\n",
       "DF           1\n",
       "NRW          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines_represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now suspect that the {1, 2, 3, 4, 5, 6} dropped out the archival service at some point, and no one noticed, because it is inexplicibly absent from three different random days now.\n",
    "\n",
    "We are going to proceed with the development process with this likelihood in mind (because we sure as hell are not going to develop our own independent archival service). When we provide the output data to users, we will do so with the note that certain trains may be absent from certain feeds. We will build an incident report for this archival service manually, by writing a reporting dashboard that takes the logified outputs as input and returns line plots of the rate counts per hour as output. This will allow us to flag when feeds drop from the archive. This will be a \"next step\" after we generate the rollups.\n",
    "\n",
    "We return to analyzing name-parts. We now know that trains may rarely end up shuffled into the wrong feed. We will ignore this problem, because this is convoluted logic I do not want in my fetch script. Instead we'll just sort by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# form a map of the form:\n",
    "# {'<$LINE_IDENTIFIER>_<$YEAR><$MONTH><$DAY>': [...list of indices in zz.namelist()]}\n",
    "# e.g.:\n",
    "# {'g_20190201': [0, 6, 12, ...],\n",
    "#  'j_20190201': [1, 7, 13, ...],\n",
    "#  ...\n",
    "# }\n",
    "name_map = defaultdict(list)\n",
    "\n",
    "for idx, n in enumerate(zz.namelist()):\n",
    "    assert 'gtfs_' in n and '.gtfs' in n\n",
    "    n = n.replace('gtfs_', '')[:-5][:-7]\n",
    "    name_map[n].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import gtfs_tripify as gt\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "AWS_BUCKET_NAME = 'gtfsarchive'\n",
    "S3_TIMEOUT = 60\n",
    "YEARMONTH = '201904'  # user-specified at runtime\n",
    "END_OF_MONTH_OVERFLOW_BUFFER_ARITY = 5 * (120)  # 5 hours, assuming 2 updates/minute\n",
    "\n",
    "\n",
    "new_york_tz = tz.gettz('America/New_York')\n",
    "current_month_first_minute = datetime(\n",
    "    int(YEARMONTH[:4]), \n",
    "    int(YEARMONTH[-2:].lstrip('0')),\n",
    "    1, \n",
    "    tzinfo=tz.gettz('America/New_York')\n",
    ")\n",
    "current_month, current_month_year = (\n",
    "    current_month_first_minute.month,\n",
    "    current_month_first_minute.year\n",
    ")\n",
    "last_minute_day_before_current_month = current_month_first_minute - timedelta(minutes=1)\n",
    "last_month, last_month_year = (\n",
    "    last_minute_day_before_current_month.month,\n",
    "    last_minute_day_before_current_month.year\n",
    ")\n",
    "next_month_overflow_time = current_month_first_minute + timedelta(\n",
    "    months=1\n",
    ")\n",
    "next_month, next_month_year = (\n",
    "    next_month_overflow_time.month,\n",
    "    next_month_overflow_time.year\n",
    ")\n",
    "CURRENT_MONTH_DOWNLOAD_URL =\\\n",
    "    f'https://s3.amazonaws.com/{AWS_BUCKET_NAME}/Data/{str(current_month)}{str(current_month_year)}.zip'\n",
    "LAST_MONTH_DOWNLOAD_URL =\\\n",
    "    f'https://s3.amazonaws.com/{AWS_BUCKET_NAME}/Data/{str(last_month)}{str(last_month_year)}.zip'\n",
    "NEXT_MONTH_DOWNLOAD_URL =\\\n",
    "    f'https://s3.amazonaws.com/{AWS_BUCKET_NAME}/Data/{str(current_month)}{str(current_month_year)}.zip'\n",
    "\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"\"\"\n",
    "    Streaming download to filename, taken from https://stackoverflow.com/a/16696317/1993206.\n",
    "    \"\"\"\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "                    # f.flush()\n",
    "    return local_filename\n",
    "\n",
    "def generate_name_map(namelist):\n",
    "    \"\"\"\n",
    "    Form a map of the form:\n",
    "    \n",
    "    {'<$LINE_IDENTIFIER>_<$YEAR><$MONTH><$DAY>': [...list of indices in namelist()], ...}\n",
    "\n",
    "    E.g.:\n",
    "\n",
    "    {'g_20190201': [0, 6, 12, ...], 'j_20190201': [1, 7, 13, ...], ...}\n",
    "    \n",
    "    This map is used to batch input to gtfs_tripify.\n",
    "    \"\"\"\n",
    "    name_map = defaultdict(list)\n",
    "    for idx, n in enumerate(namelist()):\n",
    "        assert 'gtfs_' in n and '.gtfs' in n\n",
    "        n = n.replace('gtfs_', '')[:-5][:-7]\n",
    "        name_map[n].append(idx)\n",
    "    return name_map\n",
    "\n",
    "# only need the last available file (per batch) of the last month file\n",
    "download_file(LAST_MONTH_DOWNLOAD_URL, 'last_month.zip')\n",
    "z = ZipFile('last_month.zip')\n",
    "namelist = np.array(z.namelist())\n",
    "namemap = generate_name_map(namelist)\n",
    "last_month_final_updates_namemap = {key: namemap[key][-1] for key in namename}\n",
    "for key in last_month_final_updates_namemap:\n",
    "    z.extract(last_month_final_updates_namemap[key], 'TODO: SOME ID')\n",
    "os.remove('last_month.zip')\n",
    "\n",
    "# only need the first END_OF_MONTH_OVERFLOW_BUFFER_HOURS of the next month file\n",
    "download_file(NEXT_MONTH_DOWNLOAD_URL, 'next_month.zip')\n",
    "z = ZipFile('next_month.zip')\n",
    "namelist = np.array(z.namelist())\n",
    "next_month_first_updates_namemap = {\n",
    "    key: namemap[key][:END_OF_MONTH_OVERFLOW_BUFFER_ARITY] for key in namename\n",
    "}\n",
    "os.remove('next_month.zip')\n",
    "\n",
    "# need to entire current month file\n",
    "download_file(CURRENT_MONTH_DOWNLOAD_URL, 'this_month.zip')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
